# -*- coding: utf-8 -*-
"""scoreme.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZStfRDB0yirxRdOajKmSlEPIrM3B3mr1
"""

import subprocess
subprocess.run(["pip", "install", "-U", "bitsandbytes"])

import os

os.system("pip install -U bitsandbytes")


from huggingface_hub import hf_hub_download
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import time
import psutil
import json # Import the json module

# Store your Hugging Face access token
access_token = "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"

# Model ID and filenames to be downloaded
model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
filenames = [
    "config.json", "eval_results.json", "generation_config.json",
    "model.safetensors", "special_tokens_map.json", "tokenizer.json",
    "tokenizer.model", "tokenizer_config.json"
]

# Downloading the model files
downloaded_files = {}
for filename in filenames:
    downloaded_files[filename] = hf_hub_download(
        repo_id=model_id,
        filename=filename,
        token=access_token
    )
    print(f"{filename}: {downloaded_files[filename]}")

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id, token=access_token)

# Load the configuration file and adjust the number of layers
config_path = downloaded_files["config.json"]
with open(config_path, "r") as file:
    config = json.load(file)
config["num_hidden_layers"] = 6  # Reduce the number of layers

# Load the model with the adjusted configuration
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    config=config,
    token=access_token,
    trust_remote_code=True  # Ensure remote code is trusted
)

# Move the model to CPU
device = torch.device("cpu")
model.to(device)

# Generating text using the model (on CPU)
prompt = "Hello, how are you today?"
input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(device)  # Use 'device' here
outputs = model.generate(input_ids, max_length=50)
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("Model output:", generated_text)

# Define benchmarking functions
def benchmark_inference(model, tokenizer, prompt, batch_size=1, num_runs=10):
    input_ids = tokenizer([prompt] * batch_size, return_tensors="pt", padding=True).input_ids.to(device)  # Use 'device' here

    start_time = time.time()
    with torch.no_grad():
        for _ in range(num_runs):
            _ = model.generate(input_ids)
    total_time = time.time() - start_time

    avg_inference_time = total_time / num_runs
    return avg_inference_time

def measure_memory_usage():
    process = psutil.Process()
    return process.memory_info().rss / (1024 * 1024)  # Memory in MB

# Single input inference time
single_prompt = "Hello, how are you today?"
single_inference_time = benchmark_inference(model, tokenizer, single_prompt)

# Batch inference time for 16 inputs
batch_size = 16
batch_inference_time = benchmark_inference(model, tokenizer, single_prompt, batch_size)

# Measure memory usage
memory_usage = measure_memory_usage()

# Print results
print("Single Input Inference Time:", single_inference_time, "seconds")
print("Batch Inference Time:", batch_inference_time, "seconds")
print("Memory Usage:", memory_usage, "MB")

from google.colab import drive
drive.mount('/content/drive')

save_directory = "/content/drive/My Drive/reduced_layer_model"

# Create the directory if it doesn't exist
import os
os.makedirs(save_directory, exist_ok=True)

# Save the model and tokenizer
model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

print(f"Model saved to {save_directory}")